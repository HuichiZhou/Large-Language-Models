{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 生成instruct指令数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip -q install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "正克隆到 'stanford_alpaca'...\n",
      "remote: Enumerating objects: 129, done.\u001b[K\n",
      "remote: Counting objects: 100% (73/73), done.\u001b[K\n",
      "remote: Compressing objects: 100% (23/23), done.\u001b[K\n",
      "remote: Total 129 (delta 55), reused 52 (delta 50), pack-reused 56\u001b[K\n",
      "接收对象中: 100% (129/129), 9.15 MiB | 2.67 MiB/s, done.\n",
      "处理 delta 中: 100% (61/61), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/tatsu-lab/stanford_alpaca.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/zhc/llm/My_ChatGLM_6B_Lora_Tuning_En_And_Zh/stanford_alpaca\n"
     ]
    }
   ],
   "source": [
    "%cd stanford_alpaca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: numpy in /root/anaconda3/envs/llm/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (1.25.0)\n",
      "Collecting rouge_score (from -r requirements.txt (line 2))\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting fire (from -r requirements.txt (line 3))\n",
      "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m483.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: openai in /root/anaconda3/envs/llm/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (0.27.8)\n",
      "Requirement already satisfied: transformers>=4.28.1 in /root/anaconda3/envs/llm/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (4.30.2)\n",
      "Requirement already satisfied: torch in /root/anaconda3/envs/llm/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (2.0.1)\n",
      "Requirement already satisfied: sentencepiece in /root/anaconda3/envs/llm/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (0.1.99)\n",
      "Requirement already satisfied: tokenizers>=0.13.3 in /root/anaconda3/envs/llm/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (0.13.3)\n",
      "Collecting wandb (from -r requirements.txt (line 9))\n",
      "  Downloading wandb-0.15.4-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: absl-py in /root/anaconda3/envs/llm/lib/python3.10/site-packages (from rouge_score->-r requirements.txt (line 2)) (1.4.0)\n",
      "Collecting nltk (from rouge_score->-r requirements.txt (line 2))\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.14.0 in /root/anaconda3/envs/llm/lib/python3.10/site-packages (from rouge_score->-r requirements.txt (line 2)) (1.16.0)\n",
      "Collecting termcolor (from fire->-r requirements.txt (line 3))\n",
      "  Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Requirement already satisfied: requests>=2.20 in /root/anaconda3/envs/llm/lib/python3.10/site-packages (from openai->-r requirements.txt (line 4)) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /root/anaconda3/envs/llm/lib/python3.10/site-packages (from openai->-r requirements.txt (line 4)) (4.65.0)\n",
      "Requirement already satisfied: aiohttp in /root/anaconda3/envs/llm/lib/python3.10/site-packages (from openai->-r requirements.txt (line 4)) (3.8.4)\n",
      "Requirement already satisfied: filelock in /root/anaconda3/envs/llm/lib/python3.10/site-packages (from transformers>=4.28.1->-r requirements.txt (line 5)) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /root/anaconda3/envs/llm/lib/python3.10/site-packages (from transformers>=4.28.1->-r requirements.txt (line 5)) (0.15.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /root/anaconda3/envs/llm/lib/python3.10/site-packages (from transformers>=4.28.1->-r requirements.txt (line 5)) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/anaconda3/envs/llm/lib/python3.10/site-packages (from transformers>=4.28.1->-r requirements.txt (line 5)) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /root/anaconda3/envs/llm/lib/python3.10/site-packages (from transformers>=4.28.1->-r requirements.txt (line 5)) (2023.6.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /root/anaconda3/envs/llm/lib/python3.10/site-packages (from transformers>=4.28.1->-r requirements.txt (line 5)) (0.3.1)\n",
      "Requirement already satisfied: typing-extensions in /root/anaconda3/envs/llm/lib/python3.10/site-packages (from torch->-r requirements.txt (line 6)) (4.6.3)\n",
      "Requirement already satisfied: sympy in /root/anaconda3/envs/llm/lib/python3.10/site-packages (from torch->-r requirements.txt (line 6)) (1.12)\n",
      "Requirement already satisfied: networkx in /root/anaconda3/envs/llm/lib/python3.10/site-packages (from torch->-r requirements.txt (line 6)) (3.1)\n",
      "Requirement already satisfied: jinja2 in /root/anaconda3/envs/llm/lib/python3.10/site-packages (from torch->-r requirements.txt (line 6)) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /root/anaconda3/envs/llm/lib/python3.10/site-packages (from torch->-r requirements.txt (line 6)) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /root/anaconda3/envs/llm/lib/python3.10/site-packages (from torch->-r requirements.txt (line 6)) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /root/anaconda3/envs/llm/lib/python3.10/site-packages (from torch->-r requirements.txt (line 6)) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /root/anaconda3/envs/llm/lib/python3.10/site-packages (from torch->-r requirements.txt (line 6)) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /root/anaconda3/envs/llm/lib/python3.10/site-packages (from torch->-r requirements.txt (line 6)) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /root/anaconda3/envs/llm/lib/python3.10/site-packages (from torch->-r requirements.txt (line 6)) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /root/anaconda3/envs/llm/lib/python3.10/site-packages (from torch->-r requirements.txt (line 6)) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /root/anaconda3/envs/llm/lib/python3.10/site-packages (from torch->-r requirements.txt (line 6)) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /root/anaconda3/envs/llm/lib/python3.10/site-packages (from torch->-r requirements.txt (line 6)) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /root/anaconda3/envs/llm/lib/python3.10/site-packages (from torch->-r requirements.txt (line 6)) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /root/anaconda3/envs/llm/lib/python3.10/site-packages (from torch->-r requirements.txt (line 6)) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /root/anaconda3/envs/llm/lib/python3.10/site-packages (from torch->-r requirements.txt (line 6)) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /root/anaconda3/envs/llm/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch->-r requirements.txt (line 6)) (67.8.0)\n",
      "Requirement already satisfied: wheel in /root/anaconda3/envs/llm/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch->-r requirements.txt (line 6)) (0.38.4)\n",
      "Requirement already satisfied: cmake in /root/anaconda3/envs/llm/lib/python3.10/site-packages (from triton==2.0.0->torch->-r requirements.txt (line 6)) (3.26.4)\n",
      "Requirement already satisfied: lit in /root/anaconda3/envs/llm/lib/python3.10/site-packages (from triton==2.0.0->torch->-r requirements.txt (line 6)) (16.0.6)\n",
      "Collecting Click!=8.0.0,>=7.0 (from wandb->-r requirements.txt (line 9))\n",
      "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.6/96.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting GitPython!=3.1.29,>=1.0.0 (from wandb->-r requirements.txt (line 9))\n",
      "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /root/anaconda3/envs/llm/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 9)) (5.9.5)\n",
      "Collecting sentry-sdk>=1.0.0 (from wandb->-r requirements.txt (line 9))\n",
      "  Downloading sentry_sdk-1.26.0-py2.py3-none-any.whl (209 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb->-r requirements.txt (line 9))\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Collecting pathtools (from wandb->-r requirements.txt (line 9))\n",
      "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting setproctitle (from wandb->-r requirements.txt (line 9))\n",
      "  Downloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Collecting appdirs>=1.4.3 (from wandb->-r requirements.txt (line 9))\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /root/anaconda3/envs/llm/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 9)) (4.23.3)\n",
      "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 9))\n",
      "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec in /root/anaconda3/envs/llm/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers>=4.28.1->-r requirements.txt (line 5)) (2023.6.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/anaconda3/envs/llm/lib/python3.10/site-packages (from requests>=2.20->openai->-r requirements.txt (line 4)) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/anaconda3/envs/llm/lib/python3.10/site-packages (from requests>=2.20->openai->-r requirements.txt (line 4)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/anaconda3/envs/llm/lib/python3.10/site-packages (from requests>=2.20->openai->-r requirements.txt (line 4)) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/anaconda3/envs/llm/lib/python3.10/site-packages (from requests>=2.20->openai->-r requirements.txt (line 4)) (2023.5.7)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/anaconda3/envs/llm/lib/python3.10/site-packages (from aiohttp->openai->-r requirements.txt (line 4)) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/anaconda3/envs/llm/lib/python3.10/site-packages (from aiohttp->openai->-r requirements.txt (line 4)) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /root/anaconda3/envs/llm/lib/python3.10/site-packages (from aiohttp->openai->-r requirements.txt (line 4)) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /root/anaconda3/envs/llm/lib/python3.10/site-packages (from aiohttp->openai->-r requirements.txt (line 4)) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/anaconda3/envs/llm/lib/python3.10/site-packages (from aiohttp->openai->-r requirements.txt (line 4)) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /root/anaconda3/envs/llm/lib/python3.10/site-packages (from aiohttp->openai->-r requirements.txt (line 4)) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /root/anaconda3/envs/llm/lib/python3.10/site-packages (from jinja2->torch->-r requirements.txt (line 6)) (2.1.3)\n",
      "Collecting joblib (from nltk->rouge_score->-r requirements.txt (line 2))\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /root/anaconda3/envs/llm/lib/python3.10/site-packages (from sympy->torch->-r requirements.txt (line 6)) (1.3.0)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 9))\n",
      "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Building wheels for collected packages: rouge_score, fire, pathtools\n",
      "  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=ba2a6d0803fdfe52549029545a9ccee48f5bb5bdc40515c9fc39c583c5d1ab92\n",
      "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
      "  Building wheel for fire (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116931 sha256=4ef0f7b0136bfc934c2d811361d25bb91629cd4cfdc8a8d9ae6f8fb66f95933e\n",
      "  Stored in directory: /root/.cache/pip/wheels/90/d4/f7/9404e5db0116bd4d43e5666eaa3e70ab53723e1e3ea40c9a95\n",
      "  Building wheel for pathtools (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=dc06f4eaa8b52c55997f1e54717e580cae1b56fac9f0833bcdb2915b122c6deb\n",
      "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
      "Successfully built rouge_score fire pathtools\n",
      "Installing collected packages: pathtools, appdirs, termcolor, smmap, setproctitle, sentry-sdk, joblib, docker-pycreds, Click, nltk, gitdb, fire, rouge_score, GitPython, wandb\n",
      "Successfully installed Click-8.1.3 GitPython-3.1.31 appdirs-1.4.4 docker-pycreds-0.4.0 fire-0.5.0 gitdb-4.0.10 joblib-1.2.0 nltk-3.8.1 pathtools-0.1.2 rouge_score-0.1.2 sentry-sdk-1.26.0 setproctitle-1.3.2 smmap-5.0.0 termcolor-2.3.0 wandb-0.15.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai \n",
    "openai.api_key = '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'\n",
    "os.environ['OPENAI_API_KEY'] = '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'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 以上加载完openai_key， 以下开始进行正式操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'sys' (built-in)>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import imp\n",
    "imp.reload(sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'utf-8'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.getdefaultencoding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'ascii' codec can't decode byte 0xe5 in position 210: ordinal not in range(128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtqdm\u001b[39;00m \n\u001b[0;32m---> 12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mrouge_score\u001b[39;00m \u001b[39mimport\u001b[39;00m rouge_scorer\n\u001b[1;32m     13\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mutils\u001b[39;00m \n\u001b[1;32m     15\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mfire\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.10/site-packages/rouge_score/rouge_scorer.py:40\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mre\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mabsl\u001b[39;00m \u001b[39mimport\u001b[39;00m logging\n\u001b[0;32m---> 40\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnltk\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msix\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.10/site-packages/nltk/__init__.py:133\u001b[0m\n\u001b[1;32m    125\u001b[0m     subprocess\u001b[39m.\u001b[39mPopen \u001b[39m=\u001b[39m _fake_Popen\n\u001b[1;32m    127\u001b[0m \u001b[39m###########################################################\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[39m# TOP-LEVEL MODULES\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[39m###########################################################\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \n\u001b[1;32m    131\u001b[0m \u001b[39m# Import top-level functionality into top-level namespace\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcollocations\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m    134\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdecorators\u001b[39;00m \u001b[39mimport\u001b[39;00m decorator, memoize\n\u001b[1;32m    135\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeatstruct\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.10/site-packages/nltk/collocations.py:36\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mitertools\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_itertools\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39m# these two unused imports are referenced in collocations.doctest\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     37\u001b[0m     BigramAssocMeasures,\n\u001b[1;32m     38\u001b[0m     ContingencyMeasures,\n\u001b[1;32m     39\u001b[0m     QuadgramAssocMeasures,\n\u001b[1;32m     40\u001b[0m     TrigramAssocMeasures,\n\u001b[1;32m     41\u001b[0m )\n\u001b[1;32m     42\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mspearman\u001b[39;00m \u001b[39mimport\u001b[39;00m ranks_from_scores, spearman_correlation\n\u001b[1;32m     43\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprobability\u001b[39;00m \u001b[39mimport\u001b[39;00m FreqDist\n",
      "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.10/site-packages/nltk/metrics/__init__.py:18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magreement\u001b[39;00m \u001b[39mimport\u001b[39;00m AnnotationTask\n\u001b[1;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39maline\u001b[39;00m \u001b[39mimport\u001b[39;00m align\n\u001b[0;32m---> 18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39massociation\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     19\u001b[0m     BigramAssocMeasures,\n\u001b[1;32m     20\u001b[0m     ContingencyMeasures,\n\u001b[1;32m     21\u001b[0m     NgramAssocMeasures,\n\u001b[1;32m     22\u001b[0m     QuadgramAssocMeasures,\n\u001b[1;32m     23\u001b[0m     TrigramAssocMeasures,\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconfusionmatrix\u001b[39;00m \u001b[39mimport\u001b[39;00m ConfusionMatrix\n\u001b[1;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistance\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     27\u001b[0m     binary_distance,\n\u001b[1;32m     28\u001b[0m     custom_distance,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m     presence,\n\u001b[1;32m     36\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.10/site-packages/nltk/metrics/association.py:26\u001b[0m\n\u001b[1;32m     23\u001b[0m _SMALL \u001b[39m=\u001b[39m \u001b[39m1e-20\u001b[39m\n\u001b[1;32m     25\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 26\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstats\u001b[39;00m \u001b[39mimport\u001b[39;00m fisher_exact\n\u001b[1;32m     27\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[1;32m     29\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mfisher_exact\u001b[39m(\u001b[39m*\u001b[39m_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m_kwargs):\n",
      "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/stats/__init__.py:485\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m.. _statsrefmanual:\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    480\u001b[0m \n\u001b[1;32m    481\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    483\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_warnings_errors\u001b[39;00m \u001b[39mimport\u001b[39;00m (ConstantInputWarning, NearConstantInputWarning,\n\u001b[1;32m    484\u001b[0m                                DegenerateDataWarning, FitError)\n\u001b[0;32m--> 485\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_stats_py\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m    486\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_variation\u001b[39;00m \u001b[39mimport\u001b[39;00m variation\n\u001b[1;32m    487\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdistributions\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/stats/_stats_py.py:37\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mimport\u001b[39;00m array, asarray, ma\n\u001b[1;32m     36\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlib\u001b[39;00m \u001b[39mimport\u001b[39;00m NumpyVersion\n\u001b[0;32m---> 37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtesting\u001b[39;00m \u001b[39mimport\u001b[39;00m suppress_warnings\n\u001b[1;32m     39\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mspatial\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistance\u001b[39;00m \u001b[39mimport\u001b[39;00m cdist\n\u001b[1;32m     40\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mndimage\u001b[39;00m \u001b[39mimport\u001b[39;00m _measurements\n",
      "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.10/site-packages/numpy/testing/__init__.py:11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39munittest\u001b[39;00m \u001b[39mimport\u001b[39;00m TestCase\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _private\n\u001b[0;32m---> 11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_private\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_private\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m (_assert_valid_refcount, _gen_alignment_data)\n\u001b[1;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_private\u001b[39;00m \u001b[39mimport\u001b[39;00m extbuild\n",
      "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.10/site-packages/numpy/testing/_private/utils.py:1257\u001b[0m\n\u001b[1;32m   1253\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[1;32m   1254\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m-> 1257\u001b[0m _SUPPORTS_SVE \u001b[39m=\u001b[39m check_support_sve()\n\u001b[1;32m   1259\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m   1260\u001b[0m \u001b[39m# assert_raises and assert_raises_regex are taken from unittest.\u001b[39;00m\n\u001b[1;32m   1261\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m   1262\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39munittest\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.10/site-packages/numpy/testing/_private/utils.py:1251\u001b[0m, in \u001b[0;36mcheck_support_sve\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1249\u001b[0m cmd \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mlscpu\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   1250\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1251\u001b[0m     output \u001b[39m=\u001b[39m subprocess\u001b[39m.\u001b[39;49mrun(cmd, capture_output\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, text\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m   1252\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39msve\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m output\u001b[39m.\u001b[39mstdout\n\u001b[1;32m   1253\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.10/subprocess.py:505\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[39mwith\u001b[39;00m Popen(\u001b[39m*\u001b[39mpopenargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39mas\u001b[39;00m process:\n\u001b[1;32m    504\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 505\u001b[0m         stdout, stderr \u001b[39m=\u001b[39m process\u001b[39m.\u001b[39;49mcommunicate(\u001b[39minput\u001b[39;49m, timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m    506\u001b[0m     \u001b[39mexcept\u001b[39;00m TimeoutExpired \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m    507\u001b[0m         process\u001b[39m.\u001b[39mkill()\n",
      "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.10/subprocess.py:1154\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1151\u001b[0m     endtime \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1153\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1154\u001b[0m     stdout, stderr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_communicate(\u001b[39minput\u001b[39;49m, endtime, timeout)\n\u001b[1;32m   1155\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1156\u001b[0m     \u001b[39m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[1;32m   1157\u001b[0m     \u001b[39m# See the detailed comment in .wait().\u001b[39;00m\n\u001b[1;32m   1158\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.10/subprocess.py:2043\u001b[0m, in \u001b[0;36mPopen._communicate\u001b[0;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[1;32m   2041\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_mode:\n\u001b[1;32m   2042\u001b[0m     \u001b[39mif\u001b[39;00m stdout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 2043\u001b[0m         stdout \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_translate_newlines(stdout,\n\u001b[1;32m   2044\u001b[0m                                           \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstdout\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m   2045\u001b[0m                                           \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstdout\u001b[39m.\u001b[39;49merrors)\n\u001b[1;32m   2046\u001b[0m     \u001b[39mif\u001b[39;00m stderr \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2047\u001b[0m         stderr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_translate_newlines(stderr,\n\u001b[1;32m   2048\u001b[0m                                           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr\u001b[39m.\u001b[39mencoding,\n\u001b[1;32m   2049\u001b[0m                                           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr\u001b[39m.\u001b[39merrors)\n",
      "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.10/subprocess.py:1031\u001b[0m, in \u001b[0;36mPopen._translate_newlines\u001b[0;34m(self, data, encoding, errors)\u001b[0m\n\u001b[1;32m   1030\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_translate_newlines\u001b[39m(\u001b[39mself\u001b[39m, data, encoding, errors):\n\u001b[0;32m-> 1031\u001b[0m     data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39;49mdecode(encoding, errors)\n\u001b[1;32m   1032\u001b[0m     \u001b[39mreturn\u001b[39;00m data\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m\\r\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m\\r\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'ascii' codec can't decode byte 0xe5 in position 210: ordinal not in range(128)"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json \n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "from functools import partial\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import numpy as np\n",
    "import tqdm \n",
    "from rouge_score import rouge_scorer\n",
    "import utils \n",
    "\n",
    "import fire\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Name: rouge-score\n",
      "Version: 0.1.2\n",
      "Summary: Pure python implementation of ROUGE-1.5.5.\n",
      "Home-page: https://github.com/google-research/google-research/tree/master/rouge\n",
      "Author: Google LLC\n",
      "Author-email: rouge-opensource@google.com\n",
      "License: \n",
      "Location: /root/anaconda3/envs/llm/lib/python3.10/site-packages\n",
      "Requires: absl-py, nltk, numpy, six\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatGLM-6B + LoRA "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatGLM-6B 是一个开源的、支持中英双语问答的对话语言模型，基于 General Language Model (GLM) 架构，具有 62 亿参数。  \n",
    "结合模型量化技术，用户可以在消费级的显卡上进行本地部署（INT4 量化级别下最低只需 6GB 显存）。  \n",
    "ChatGLM-6B 使用了和 ChatGLM 相同的技术，针对中文问答和对话进行了优化。  \n",
    "经过约 1T 标识符的中英双语训练，辅以监督微调、反馈自助、人类反馈强化学习等技术的加持，62 亿参数的 ChatGLM-6B 已经能生成相当符合人类偏好的回答。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "823a00a06b6c45cc9db1b02bb7519d41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True).half().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7099b482f1474c44bf86edad5237bc85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from typing import Dict, Tuple, Union, Optional\n",
    "\n",
    "from torch.nn import Module\n",
    "\n",
    "def auto_configure_device_map(num_gpus: int) -> Dict[str, int]:\n",
    "    # transformer.word_embeddings 占用1层\n",
    "    # transformer.final_layernorm 和 lm_head 占用1层\n",
    "    # transformer.layers 占用 28 层\n",
    "    # 总共30层分配到num_gpus张卡上\n",
    "    num_trans_layers = 28\n",
    "    per_gpu_layers = 30 / num_gpus\n",
    "\n",
    "    # bugfix: 在linux中调用torch.embedding传入的weight,input不在同一device上,导致RuntimeError\n",
    "    # windows下 model.device 会被设置成 transformer.word_embeddings.device\n",
    "    # linux下 model.device 会被设置成 lm_head.device\n",
    "    # 在调用chat或者stream_chat时,input_ids会被放到model.device上\n",
    "    # 如果transformer.word_embeddings.device和model.device不同,则会导致RuntimeError\n",
    "    # 因此这里将transformer.word_embeddings,transformer.final_layernorm,lm_head都放到第一张卡上\n",
    "    device_map = {'transformer.word_embeddings': 0,\n",
    "                  'transformer.final_layernorm': 0, \n",
    "                  'lm_head': 0}\n",
    "\n",
    "    used = 2\n",
    "    gpu_target = 0\n",
    "    for i in range(num_trans_layers):\n",
    "        if used >= per_gpu_layers:\n",
    "            gpu_target += 1\n",
    "            used = 0\n",
    "        assert gpu_target < num_gpus\n",
    "        device_map[f'transformer.layers.{i}'] = gpu_target\n",
    "        used += 1\n",
    "\n",
    "    return device_map\n",
    "\n",
    "def load_model_on_gpus(checkpoint_path: Union[str, os.PathLike], num_gpus: int = 2,\n",
    "                       device_map: Optional[Dict[str, int]] = None, **kwargs) -> Module:\n",
    "    if num_gpus < 2 and device_map is None:\n",
    "        model = AutoModel.from_pretrained(checkpoint_path, trust_remote_code=True, **kwargs).half().cuda()\n",
    "    else:\n",
    "        from accelerate import dispatch_model\n",
    "\n",
    "        model = AutoModel.from_pretrained(checkpoint_path, trust_remote_code=True, **kwargs).half()\n",
    "\n",
    "        if device_map is None:\n",
    "            device_map = auto_configure_device_map(num_gpus)\n",
    "\n",
    "        model = dispatch_model(model, device_map=device_map)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = load_model_on_gpus(\"THUDM/chatglm-6b\", num_gpus=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The dtype of attention mask (torch.int64) is not bool\n"
     ]
    }
   ],
   "source": [
    "response, history = model.chat(tokenizer, \"你好\", history=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'你好👋！我是人工智能助手 ChatGLM-6B，很高兴见到你，欢迎问我任何问题。'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "response, history = model.chat(tokenizer, \"现在怎么入手nlp?\", history=history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NLP 是自然语言处理(Natural Language Processing)的缩写，是一种人工智能技术，旨在使计算机理解和解释人类语言。如果您想入手 NLP，以下是一些步骤：\\n\\n1. 学习自然语言处理的基础知识：了解自然语言处理的基本概念、技术和应用，例如文本预处理、文本分类、命名实体识别、情感分析、机器翻译等。\\n\\n2. 掌握编程技能：自然语言处理需要编程技能，因此您需要掌握至少一门编程语言，例如 Python、Java 等。\\n\\n3. 学习 NLP 框架：了解常用的自然语言处理框架，例如 NLTK、 spaCy、 Gensim 等。\\n\\n4. 实践项目：通过实践项目来巩固所学知识，例如通过构建一个简单的文本分类器、情感分析系统等。\\n\\n5. 参加在线课程或培训：参加在线课程或培训可以为您提供专业的指导和学习资源，例如Coursera、Udacity、edX等。\\n\\n6. 加入社区：加入自然语言处理相关的社区可以为您提供与其他学习者和专业人士交流的机会，例如 Stack Overflow、GitHub 等。\\n\\n希望这些步骤可以帮助您入门自然语言处理领域。'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "response, history = model.chat(tokenizer, \"我来自华北电力大学\", history=history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'很高兴听到您来自华北电力大学！作为一个人工智能助手，我可以回答您的问题，提供帮助和信息。如果您有任何问题，请随时问我。'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "response, history = model.chat(tokenizer, \"按照我上面告诉你的信息，我来自哪儿\", history=history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'根据您提供的信息，您来自华北电力大学。作为一个人工智能助手，我无法验证您提供的信息的准确性，但我希望这些信息对您有所帮助。如果您有任何问题，请随时问我。'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "电力系统是指将电能从发电站传输到用电设备的系统，是现代社会电力供应的基础。以下是电力系统的一些评价方面：\n",
       "\n",
       "1. 可靠性：电力系统的可靠性对于电力供应的稳定性至关重要。电力系统的设计和运行需要考虑到各种因素，如发电站和用电设备的故障率、电力传输的延迟和中断等，确保电力系统的可靠运行。\n",
       "\n",
       "2. 效率：电力系统的效率指的是其能量利用效率，即电力系统将能量从发电站传输到用电设备的效率。高效的电力系统可以减少能源浪费和损失，提高能源利用效率。\n",
       "\n",
       "3. 可持续性：电力系统的可持续性需要考虑其环境影响和资源利用。现代电力系统通常采用可再生能源发电，以减少对化石燃料的依赖和对环境的污染。同时，电力系统的可持续性还考虑其能源消耗和能源储存的可持续性。\n",
       "\n",
       "4. 安全性：电力系统的安全性包括各种安全措施，如电力保障、人员安全、设备保护等，以确保电力系统的正常运行和安全运行。\n",
       "\n",
       "5. 成本效益：电力系统的成本效益需要考虑其经济效益，即电力系统的能源成本、建设成本、运营成本等与能源价格的关系。高能源价格可能导致电力系统成本上升，但低能源价格则可能导致电力系统成本下降。\n",
       "\n",
       "电力系统是一个复杂的系统，需要考虑多个方面的评价，包括可靠性、效率、可持续性、安全性和成本效益等。"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown, clear_output\n",
    "\n",
    "prompt = \"从多方面评价电力系统\"\n",
    "for response, history in model.stream_chat(\n",
    "        tokenizer, prompt, history=[]):\n",
    "    clear_output(wait=True)\n",
    "    display(Markdown(response))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eb35025f1104b43aec35c358ec0a421",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "add670512a3847b8beafb1e7aa9e487f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "260c664749754c5d899c0ff479aa8be1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05fdfb5a6af4483f8deb0eaa1cacbfb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline('sentiment-analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
